\documentclass{article}
% \usepackage[headsepline]{scrlayer-scrpage}

% \ihead{Levin}
% \ohead{\thepage}
% \pagestyle{scrheadings}
\usepackage{bbm}
\usepackage{amsmath,amsfonts,amsthm,amssymb,amsopn,bm}
\usepackage[margin=.9in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage[usenames,dvipsnames]{color}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{pythonhighlight}
\usepackage{pgfplots}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\usepackage{minted}
% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

% Some additional tweaking for this package can be made in the preamble. To change the size of each plot and also guarantee backwards compatibility (recommended) add the next line:

\pgfplotsset{width=10cm,compat=1.9}

% This changes the size of each pgfplot figure to 10 centimeters, which is huge; you may use different units (pt, mm, in). The compat parameter is for the code to work on the package version 1.9 or later.

% Since LaTeX was not initially conceived with plotting capabilities in mind, when there are several pgfplot figures in your document or they are very complex, it takes a considerable amount of time to render them. To improve the compiling time you can configure the package to export the figures to separate PDF files and then import them into the document, add the code shown below to the preamble:

\usepgfplotslibrary{external}

\tikzexternalize 

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\I}{\mathbbm{1}}
\newcommand{\E}{\mathbb{E}} 
\newcommand{\V}{\mathbb{V}} 
\renewcommand{\P}{\mathbb{P}}
 \newcommand{\ind}{\perp\!\!\!\perp}
 \DeclareMathOperator{\rank}{rank}
\newcommand{\R}{\field{R}} % real domain
% \newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain

\newcommand{\T}{^{\textrm T}} % transpose

\def\diag{\text{diag}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}
\renewcommand{\theenumi}{\alph{enumi}} 

\newcommand{\Perp}{\perp \! \! \! \perp}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\cst}[1]{\mathsf{#1}} % constant
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand{\points}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}
\date{{}}

\setlength\parindent{0px}

\begin{document}
\title{Homework \#3}
\author{\normalsize{Spring 2020, CSE 546: Machine Learning}\\
\normalsize{\bf Roman Levin} \\
\normalsize{\bf 1721898} \\
}
\maketitle
Collaborators: compared answers with Tyler Chen, Diya Sashidhar, Katherine Owens

\subsection*{Conceptual Questions}
\noindent\rule{\textwidth}{1pt}

A.1 {\bf Solution:}\\
\begin{enumerate}
    \item {\bf True} (yes, since only $k$ eigenvalues of the covariance matrix are nonzero)
    \item {\bf False} (SVM just maximizes the margin, it does not mean the generalization error is optimal among linear models)
    \item {\bf True} (since the bootstrap sampling is with replacement)
    \item {\bf False} (the columns of $V$ are the eigenvectors, not the rows)
    \item {\bf False} (not necessarily, new PCA coordinates could sometimes be very uninterpretable)
    \item {\bf False} ($k=n$ where $n$ is the number of samples results in zero objective and in only one point belonging to each cluster giving no new insights into the data)
    \item {\bf Decrease $\sigma$} (Decreasing $\sigma$ makes the model more expressive.)
\end{enumerate}

\noindent\rule{\textwidth}{1pt}
\subsection*{Kernels and the Bootstrap}
\noindent\rule{\textwidth}{1pt}
A.2 {\bf Solution:}\\
By definition of $\phi(x)$, we have:
$$
\boxed{\phi(x)\phi(x') = \sum_{i=0}^\infty \frac{1}{\sqrt{i!}}e^{-\frac{x^2}{2}}x^i \frac{1}{\sqrt{i!}}e^{-\frac{x'^2}{2}}x'^i =  e^{-\frac{x^2+x'^2}{2}}\sum_{i=0}^\infty \frac{1}{i!}(xx')^i = e^{-\frac{x^2+x'^2}{2}}e^{xx'} = e^{-\frac{(x-x')^2}{2}} \qquad \Box}
$$
\noindent\rule{\textwidth}{1pt}

\noindent\rule{\textwidth}{1pt}
\\
A.3 {\bf Solution:}\\
\begin{enumerate}
    \item Best polynomial kernel parameters: $\lambda = 0.001, d = 17.0$\\
    Best rbf kernel parameters: $\lambda = 0.0001, \gamma = 5.340266090876008$
    \item See Figure 1.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3b_poly.pdf}
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3b_rbf.pdf}
            \caption{Problem A3.b Left: Polynomial kernel model, Right: Rbf kernel model}
            \label{figure:a4}
        \end{figure}
    \item See Figure 2.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3c_poly.pdf}
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3c_rbf.pdf}
            \includegraphics[width=0.6\textwidth]{hw3/code/figures/A3c_poly_zoomed.pdf}
            \caption{Problem A3.c Bootstrap Confidence Intervals. Top Left: Polynomial kernel model, Top Right: Rbf kernel model, Bottom: Zoomed-in version of the polynomial kernel graph, since on the boundaries it is very unstable.}
            \label{figure:a4}
        \end{figure}
    \item Best polynomial kernel parameters for $n=300$: $\lambda = 0.001, d = 16.0$\\
    Best rbf kernel parameters for $n=300$: $\lambda = 0.01, \gamma =  5.131332212328231$\\
    See Figures 3 and 4.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3db_poly.pdf}
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3db_rbf.pdf}
            \caption{Problem A3.d Left: Polynomial kernel model, Right: Rbf kernel model}
            \label{figure:a4}
        \end{figure}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3dc_poly.pdf}
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A3dc_rbf.pdf}
            \caption{Problem A3.d Bootstrap Confidence Intervals. Left: Polynomial kernel model, Right: Rbf kernel model.}
            \label{figure:a4}
        \end{figure}
    \item Expected Squared Error Difference 90\% bootstrap CI: [0.028427236202594672, 0.078246629760424]. We see that zero is not inside this interval which suggests that rbf kernel is better for predicting $Y$ from $X$ since in 90\% of the bootstrap cases, we saw that the rbf kernel model average error was lower than that of the polynomial kernel model. We also saw above that the polynomial kernel is very unstable.
    \\
    \\
    See the code for this problem below.
\end{enumerate}  

\inputminted{python}{code/A3.py}
\caption{Code for A3}\\

\noindent\rule{\textwidth}{1pt}
\\
\subsection*{k-means clustering}
\noindent\rule{\textwidth}{1pt}
\\
A.4 {\bf Solution:}\\
\begin{enumerate}
    \item \inputminted{python}{code/A4.py}
          \caption{Code for A4}
    \item See Figure 5.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.44\textwidth]{hw3/code/figures/A4b_obj.pdf}
            \includegraphics[width=0.55\textwidth]{hw3/code/figures/A4b_centroids.pdf}
            \caption{Problem A4.b Left: K-means Objective, Right: Centroids}
        \end{figure}
    \item
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.7\textwidth]{hw3/code/figures/A4c_.pdf}
            \caption{Problem A4.c Training and Test Errors as a function of $k$}
        \end{figure}
\end{enumerate}   
\noindent\rule{\textwidth}{1pt}

\subsection*{Intro to Sample Complexity}
\noindent\rule{\textwidth}{1pt}
\\
B.1 {\bf Solution:}\\
\begin{enumerate}
    \item Using the fact that $R(f) > \epsilon$. Using the definitions and the fact that the expectation of an indicator of an event is the probability of the event and using the fact that samples are iid:
    \begin{align}
    & \P(\hat{R}_n(f) = 0) = \P(\frac{1}{n}\sum_{i=1}^n \I(f(x_i) \not = y_i)) = \P(\forall i: f(x_i) = y_i) = \prod_{i=1}^n \P(f(x_i)=y_i) =  \prod_{i=1}^n (1-\P(f(x_i)\not = y_i)) =  \\
    &= (1-\P(f(x_1)\not = y_1))^n = (1 - \E_{XY}[\I(f(X) \not = Y)])^n = (1 - R(f))^n \le (1-\epsilon)^n \le e^{-n\epsilon} \qquad \Box
    \end{align}
    \item For every $f \in \mathcal{F}$, define $A_f = \{R(f)>\epsilon \text{ and } \hat{R}_n(f) = 0 \}$. Note that $\P(\{R(f)>\epsilon \text{ and } \hat{R}_n(f) = 0 \}) = \P(\hat{R}_n(f) = 0|R(f)>\epsilon)\P(R(f)>\epsilon) \le \P(\hat{R}_n(f) = 0|R(f)>\epsilon)$ and that $\forall f \quad \P(A_f) \le e^{-\epsilon n}$ follows from a. Then:
    $$
    \P(\exists f \in \mathcal{F}: R(f) > \epsilon \text{ and } \hat{R}_n(f) = 0) \le \P(\cup_{f \in \mathcal{F}} A_f) \le \sum_{f \in \mathcal{F}} \P(A_f) \le |\mathcal{F}|e^{-\epsilon n} \qquad \Box
    $$
    \item $$
    |\mathcal{F}|e^{-\epsilon n} \le \delta \Leftrightarrow \epsilon \ge \frac{1}{n}\log \frac{\mathcal{|F|}}{\delta} \Rightarrow \boxed{\epsilon^* = \frac{1}{n}\log \frac{\mathcal{|F|}}{\delta}}
    $$
    \item Note: B1.d is a vaguely worded question. To solve it, let $A$ and $B$ be two boolean events. Then $A \Rightarrow B$ is also a boolean event. Taking into account the truth table for $A \Rightarrow B$, observe $\P(\{A \Rightarrow B \text{ is true}\}) = 1 - \P(\{A \text{ is true}\}  \cap \{B  \text{ is false}\})$. For short,  $\P(A \Rightarrow B) = 1 - \P(A \cap \neg B)$. Now, take $A = \{\hat{R}_n(f) = 0\}$ and $B = \{R(f) - R(f^*) \le \frac{1}{n}\log \frac{\mathcal{|F|}}{\delta}\} =c $ (omitting hats over $f$ for brevity since any function with zero empirical risk automatically becomes its argmin, since the empirical risk is non-negative). Finally, for $A$ and $B$ defined above (and since $R(f^*) \ge 0$) and using previous parts:
    \begin{align}
        &\P(A \Rightarrow B) = 1 - \P(A \cap \neg B) = 1 - \P(\hat{R}_n(f) = 0 \text{ and } R(f) - R(f^*) > \epsilon^*) \ge \\
        & \ge 1 - \P(\hat{R}_n(f) = 0 \text{ and } R(f) > \epsilon^*) \ge
        1 - \P(\exists f \in \mathcal{F}: R(f) > \epsilon^* \text{ and } \hat{R}_n(f) = 0) \ge \\
        &\ge 1 - |\mathcal{F}|e^{-\epsilon^* n} \ge 1 - \delta \qquad \Box
    \end{align}
    \end{enumerate}   
\noindent\rule{\textwidth}{1pt}

\subsection*{Neural Network for MNIST}
\noindent\rule{\textwidth}{1pt}
\\
A.5 {\bf Solution:}\\
\begin{enumerate}
    \item The wide network was trained using Adam with learning rate 0.001 and converged in 18 epochs (achieved 99\% accuracy on the training data). After 18 training epochs we got the following results:
    \begin{itemize}
        \item Training loss: {\bf 0.0348}
        \item Test loss: {\bf 0.0923}
        \item Test accuracy: {\bf 0.9730}
    \end{itemize}
    See Figure 7 for the training plot. 
    
    \item The deep network was trained using Adam with learning rate 0.001 and converged in 35 epochs (achieved 99\% accuracy on the training data). After 35 training epochs we got the following results:
    \begin{itemize}
        \item Training loss: {\bf 0.0310}
        \item Test loss: {\bf 0.1302}
        \item Test accuracy: {\bf 0.9670}
    \end{itemize}
    See Figure 7 for the training plot. 
    
    \item There are {\bf 26506} parameters for the deep network and {\bf 50890} parameters for the wide network. At the same time there is a tiny difference in test accuracy for wide and deep networks. Even though wide network required less epochs to converge, deep network has almost twice fewer trainable parameters which makes it computationally less expensive. Therefore, I would say the deep network is better. Even though the one-hidden-layer network is already a universal function approximator, that one layer requires much more trainable parameters than a deep network.
    
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.84\textwidth]{hw3/code/figures/A5_training_plots.pdf}
    \caption{Problem A5 a and b: Training plot for both wide and deep networks}
\end{figure}
\end{enumerate}   
\inputminted{python}{code/A5.py}
\caption{Code for A5}\\
\noindent\rule{\textwidth}{1pt}

\subsection*{PCA}
\noindent\rule{\textwidth}{1pt}
\\
A.6 {\bf Solution:}\\
\begin{enumerate}
    \item The requested eigvals: [332719.12203544, 243279.88433819, 80808.47770154,  23685.72821113,  11035.29046611] \\
The sum of eigvals: 3428445.433070634
    \item $$
    x^T \approx \mu^T + (x-\mu)^TV_kV_k^T, \text{  where columns of $V_k$ are the $k$  eigenvectors corresponding to the $k$ largest eigenvalues.}
    $$
    \item See Figure 8.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A6c_errors.pdf}
            \includegraphics[width=0.49\textwidth]{hw3/code/figures/A6c_variance.pdf}
            \caption{Problem A6.c Left: Reconstruction errors, Right: The amount of unexplained variance.}
        \end{figure}
    \item See Figure 9. Looks like the eigenvectors are capturing some basic shapes commonly present in digits. That is, the leading PCA eigenmode (PCA Mode 0) seems to capture ovals like those in zeros. The second leading PCA eigenmode (PCA Mode 1) captures edges similar to those present in 4 and 9, the third eigenmode (PCA Mode 2) seems to be related to 8 and 3, etc.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.8\textwidth]{hw3/code/figures/A6d_modes.pdf}
            \caption{Problem A6.d: The top 10 leading PCA eigenmodes.}
        \end{figure}
    \item See Figure 10. Of course, we see that the more eigenmodes we use for reconstruction -- the better quality reconstructions we get, that is, the higher-rank approximations are more accurate which is expected. However, we can also see that the reconstructions for as few as 40 eigenmodes is already decent for all the digits, even though we only use 40 out of 784 eigenmodes. For 7, even 5 eigenmodes are already capturing the digit well.
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.8\textwidth]{hw3/code/figures/A6e_2.pdf}
            \includegraphics[width=0.8\textwidth]{hw3/code/figures/A6e_6.pdf}
            \includegraphics[width=0.8\textwidth]{hw3/code/figures/A6e_7.pdf}
            \caption{Problem A6.e: Reconstruction for different k. The originals are on the left. Top: Reconstructions for 2. Middle: Reconstructions for 6. Bottom: Reconstructions for 7.}
        \end{figure}
\end{enumerate}   
\noindent\rule{\textwidth}{1pt}



\end{document}
